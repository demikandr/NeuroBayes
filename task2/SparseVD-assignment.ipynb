{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Практичиское задание 2: Sparse Variational Dropout</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит реализовать Sparse VD -- метод для разреживания нейронных сетей https://arxiv.org/abs/1701.05369  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from logger import Logger\n",
    "from torch.nn import Parameter\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализуем полносвязный Sparse VD слой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSVDO(nn.Module):\n",
    "    def __init__(self, in_features, out_features, threshold, bias=True):\n",
    "        super(LinearSVDO, self).__init__()\n",
    "        # in_features int\n",
    "        # out_features int \n",
    "        # threshold float\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.threshold = threshold\n",
    "\n",
    "        # =======================================\n",
    "        # Создайте параметры модели -- объекты класса Parameter\n",
    "        # W размера (out_features x in_features)\n",
    "        # log_sigma размера (out_features x in_features)\n",
    "        # bias размера (1, out_features)\n",
    "        # =======================================\n",
    "        self.W = Parameter(torch.rand((self.out_features, self.in_features)) - 0.5) * 2 / self.in_features ** 0.5\n",
    "        self.log_sigma = Parameter(torch.zeros((self.out_features, self.in_features)) - 5)\n",
    "        self.bias = Parameter(torch.zeros((1, self.out_features)))\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # =======================================\n",
    "        # Инициализируйте параметры модели\n",
    "        # W -- нормальный случайный шум с центром в 0 и маленькой дисперсией\n",
    "        # log_sigma -- маленьким значением ~ -5 \n",
    "        # bias -- можно 0\n",
    "        # =======================================   \n",
    "        self.W = Parameter(torch.randn((self.out_features, self.in_features)))\n",
    "        self.log_sigma = Parameter(torch.zeros((self.out_features, self.in_features)) - 5)\n",
    "        self.bias = Parameter(torch.zeros((1, self.out_features)))\n",
    "        \n",
    "        stdv = 1. / math.sqrt(self.W.size(1))\n",
    "        self.W.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "        self.log_sigma.data.uniform_(-5, -5)\n",
    "    \n",
    "    @property\n",
    "    def log_alpha(self):\n",
    "        return 2 * self.log_sigma - (self.W ** 2 + 1e-8).log()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # =======================================\n",
    "        # x: Variable containing: [torch.FloatTensor of size batch_size x in_features]\n",
    "        # Return: type: Variable containing [torch.FloatTensor of size batch_size x out_features]\n",
    "        # ----------------------------------------\n",
    "        # Тут нужно написать forward шаг для Sparse VD слоя для минибатча объектов x\n",
    "        # На этапе обучения: Вернуть семпл активаций с помощью Local Reparametrization Trick \n",
    "        # На этапе тестирования: Вернуть средние активации, посчитанные с обрезанными весами \n",
    "        # Правило обрезания весов: alpha_ij < self.threshold ====> w_ij = 0\n",
    "        # ----------------------------------------\n",
    "        # Клипинг alpha_ij, например torch.clamp(self.log_alpha, -10, 10) может улучшить стабильность \n",
    "        # Чтобы не встретить nan-ы используйте трюк log(a) = log(a + 1e-8)\n",
    "        # ======================================= \n",
    "        \n",
    "        n, _ = x.shape\n",
    "        if self.training: # training\n",
    "            eps = Variable(torch.randn((n, self.out_features))) \n",
    "            output = F.linear(x, self.W, self.bias) + \\\n",
    "            eps * F.linear(x ** 2, self.log_sigma.clamp(-10, 10).exp() ** 2) ** 0.5\n",
    "        else:\n",
    "            w = self.W * (self.log_alpha.exp() < (self.threshold + 1e-8)).float()\n",
    "            output = F.linear(x, w, self.bias)\n",
    "        return output\n",
    "        \n",
    "    def kl_reg(self):\n",
    "        # =======================================\n",
    "        # Вернуть суммарную KL дивергенцию для всех параметров слоя \n",
    "        # Return: Variable containing: [torch.FloatTensor of size 1]\n",
    "        # =======================================\n",
    "        k1 = 0.63576\n",
    "        k2 = 1.87320\n",
    "        k3 = 1.48695\n",
    "        C = -k1\n",
    "        kl = -(k1 * torch.sigmoid(k2 + k3 * self.log_alpha) - 0.5 * (1 + (-self.log_alpha).exp()).log() + C)\n",
    "        return kl.sum(dim=0).sum(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создадим простую архитектуру LeNet-300-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, threshold):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = LinearSVDO(28*28, 300, threshold)\n",
    "        self.fc2 = LinearSVDO(300,  100, threshold)\n",
    "        self.fc3 = LinearSVDO(100,  10, threshold)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.log_softmax(self.fc3(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузим MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist(batch_size):\n",
    "    trsnform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "        transform=trsnform), batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, download=True,\n",
    "        transform=trsnform), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Определим новую функцию потерь SGVLB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGVLB(nn.Module):\n",
    "    def __init__(self, net, train_size):\n",
    "        super(SGVLB, self).__init__()\n",
    "        self.train_size = train_size\n",
    "        self.net = net\n",
    "\n",
    "    def forward(self, output, target, kl_weight=1.0):\n",
    "        # =======================================\n",
    "        # output -- ответы модели для минибатча [torch.FloatTensor of size batch_size x 10]\n",
    "        # target -- настоящие ответы  [torch.LongTensor of size batch_size]\n",
    "        # kl_weight -- коэффициент на который нужно умножить kl дивергенцию, нужен для отжига (читай ниже)\n",
    "        # Вернуть Variable с посчитанной SGVLB функцией потерь \n",
    "        # Используйте self.net.children() для обхода всех слоев модели\n",
    "        # !!! Проверьте что множитель перед data term правильный !!!\n",
    "        # Return: Variable containing: [torch.FloatTensor of size 1]\n",
    "        # =======================================\n",
    "        loss = Variable(torch.FloatTensor([0]))\n",
    "        for layer in self.net.children():\n",
    "            loss += kl_weight * layer.kl_reg()\n",
    "        loss += torch.nn.CrossEntropyLoss(size_average=False)(output, target) * self.train_size / target.shape[0]\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs, batch_size, threshold = 100, 100, 3\n",
    "model = Net(threshold)\n",
    "optimizer = torch.optim.Adam(model.parameters())# Тут ваш любимый оптимизатор, адам -- хороший выбор\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 5, gamma=0.9) # Тут расписание шага обучения torch.optim.lr_scheduler\n",
    "fmt = {'tr_los': '3.1e', 'te_loss': '3.1e', 'sp_0': '.3f', 'sp_1': '.3f', 'lr': '3.1e', 'kl': '.2f'}\n",
    "logger = Logger('sparse_vd', fmt=fmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.14984130859375e-05\n",
      "  epoch    kl       lr    tr_los    tr_acc    te_loss    te_acc    sp_0    sp_1    sp_2\n",
      "-------  ----  -------  --------  --------  ---------  --------  ------  ------  ------\n",
      "      1  0.04  1.0e-03   2.5e+02      92.3    1.3e+02      96.4   0.352   0.131     0.0\n",
      "31.576680183410645\n",
      "      2  0.06  1.0e-03   1.4e+02      96.7    1.1e+02      97.4   0.464   0.209     0.0\n",
      "63.64189958572388\n",
      "      3  0.08  1.0e-03   1.2e+02      97.4    1.0e+02      97.7   0.555   0.299     0.1\n",
      "93.79713273048401\n",
      "      4  0.10  1.0e-03   1.1e+02      97.8    9.9e+01      97.6   0.625   0.347     0.1\n",
      "123.9317524433136\n",
      "      5  0.12  1.0e-03   9.9e+01      97.9    9.3e+01      98.0   0.677   0.414     0.1\n",
      "155.2494010925293\n",
      "      6  0.14  9.0e-04   8.9e+01      98.2    8.6e+01      98.2   0.753   0.474     0.1\n",
      "187.20657110214233\n",
      "      7  0.16  9.0e-04   8.7e+01      98.2    8.8e+01      98.1   0.779   0.515     0.1\n",
      "217.05747842788696\n",
      "      8  0.18  9.0e-04   8.5e+01      98.3    8.6e+01      98.3   0.794   0.546     0.1\n",
      "248.51222777366638\n",
      "      9  0.20  9.0e-04   8.5e+01      98.3    8.3e+01      98.4   0.823   0.578     0.1\n",
      "278.4587981700897\n",
      "     10  0.22  9.0e-04   8.5e+01      98.3    8.6e+01      98.3   0.828   0.595     0.1\n",
      "309.7381680011749\n",
      "     11  0.24  8.1e-04   8.1e+01      98.5    8.4e+01      98.3   0.831   0.607     0.1\n",
      "341.3376133441925\n",
      "     12  0.26  8.1e-04   8.2e+01      98.4    8.4e+01      98.5   0.844   0.617     0.1\n",
      "376.0199718475342\n",
      "     13  0.28  8.1e-04   8.3e+01      98.4    8.4e+01      98.4   0.855   0.648     0.2\n",
      "407.3750693798065\n",
      "     14  0.30  8.1e-04   8.3e+01      98.4    8.4e+01      98.4   0.871   0.673     0.2\n",
      "439.1951768398285\n",
      "     15  0.32  8.1e-04   8.4e+01      98.4    8.5e+01      98.3   0.863   0.651     0.2\n",
      "470.65594720840454\n",
      "     16  0.34  7.3e-04   8.2e+01      98.5    8.4e+01      98.5   0.881   0.689     0.2\n",
      "500.535715341568\n",
      "     17  0.36  7.3e-04   8.3e+01      98.4    8.3e+01      98.6   0.872   0.686     0.2\n",
      "530.2811725139618\n",
      "     18  0.38  7.3e-04   8.3e+01      98.4    8.4e+01      98.5   0.893   0.720     0.2\n",
      "562.017648935318\n",
      "     19  0.40  7.3e-04   8.2e+01      98.5    8.6e+01      98.4   0.890   0.710     0.2\n",
      "591.9645752906799\n",
      "     20  0.42  7.3e-04   8.5e+01      98.4    8.6e+01      98.5   0.890   0.719     0.2\n",
      "623.2814428806305\n",
      "     21  0.44  6.6e-04   8.5e+01      98.4    8.6e+01      98.3   0.901   0.741     0.2\n",
      "653.6818134784698\n",
      "     22  0.46  6.6e-04   8.4e+01      98.5    8.5e+01      98.5   0.909   0.758     0.2\n",
      "683.7136671543121\n",
      "     23  0.48  6.6e-04   8.5e+01      98.4    8.4e+01      98.4   0.911   0.763     0.3\n",
      "714.0105862617493\n",
      "     24  0.50  6.6e-04   8.5e+01      98.4    8.5e+01      98.4   0.913   0.771     0.2\n",
      "744.4767112731934\n",
      "     25  0.52  6.6e-04   8.7e+01      98.3    8.6e+01      98.5   0.916   0.774     0.3\n",
      "774.2342572212219\n",
      "     26  0.54  5.9e-04   8.5e+01      98.4    8.5e+01      98.5   0.924   0.807     0.3\n",
      "804.8937125205994\n",
      "     27  0.56  5.9e-04   8.7e+01      98.3    8.6e+01      98.5   0.922   0.803     0.3\n",
      "836.400842666626\n",
      "     28  0.58  5.9e-04   8.6e+01      98.4    8.7e+01      98.4   0.926   0.816     0.3\n",
      "866.5044887065887\n",
      "     29  0.60  5.9e-04   8.8e+01      98.4    8.7e+01      98.5   0.928   0.819     0.3\n",
      "896.6101474761963\n",
      "     30  0.62  5.9e-04   8.8e+01      98.3    8.7e+01      98.4   0.931   0.825     0.3\n",
      "928.1059460639954\n",
      "     31  0.64  5.3e-04   8.7e+01      98.4    8.8e+01      98.3   0.933   0.831     0.3\n",
      "959.7438554763794\n",
      "     32  0.66  5.3e-04   8.9e+01      98.3    8.7e+01      98.4   0.938   0.850     0.3\n",
      "991.5242853164673\n",
      "     33  0.68  5.3e-04   8.8e+01      98.3    8.8e+01      98.4   0.938   0.847     0.3\n",
      "1022.1991677284241\n",
      "     34  0.70  5.3e-04   8.9e+01      98.4    8.7e+01      98.4   0.940   0.854     0.3\n",
      "1052.3331880569458\n",
      "     35  0.72  5.3e-04   9.0e+01      98.3    9.0e+01      98.4   0.937   0.846     0.4\n",
      "1082.2178440093994\n",
      "     36  0.74  4.8e-04   9.0e+01      98.3    8.8e+01      98.4   0.944   0.865     0.4\n",
      "1115.274930715561\n",
      "     37  0.76  4.8e-04   9.1e+01      98.3    8.9e+01      98.3   0.944   0.867     0.4\n",
      "1145.0566926002502\n",
      "     38  0.78  4.8e-04   9.2e+01      98.2    9.0e+01      98.3   0.944   0.870     0.4\n",
      "1173.45632314682\n",
      "     39  0.80  4.8e-04   9.3e+01      98.2    8.9e+01      98.4   0.945   0.872     0.4\n",
      "1203.402792930603\n",
      "     40  0.82  4.8e-04   9.2e+01      98.2    8.9e+01      98.2   0.948   0.884     0.4\n",
      "1233.3240129947662\n",
      "     41  0.84  4.3e-04   9.2e+01      98.3    9.0e+01      98.3   0.950   0.888     0.4\n",
      "1263.2591662406921\n",
      "     42  0.86  4.3e-04   9.2e+01      98.3    9.0e+01      98.3   0.950   0.887     0.4\n",
      "1292.7655322551727\n",
      "     43  0.88  4.3e-04   9.3e+01      98.2    9.0e+01      98.3   0.952   0.892     0.4\n",
      "1322.0008776187897\n",
      "     44  0.90  4.3e-04   9.4e+01      98.2    9.1e+01      98.3   0.951   0.892     0.4\n",
      "1350.5259997844696\n",
      "     45  0.92  4.3e-04   9.5e+01      98.2    9.1e+01      98.2   0.954   0.895     0.4\n",
      "1382.1315529346466\n",
      "     46  0.94  3.9e-04   9.3e+01      98.3    9.2e+01      98.2   0.955   0.902     0.4\n",
      "1412.198992729187\n",
      "     47  0.96  3.9e-04   9.4e+01      98.2    9.2e+01      98.2   0.956   0.902     0.4\n",
      "1440.1694622039795\n",
      "     48  0.98  3.9e-04   9.6e+01      98.2    9.2e+01      98.3   0.956   0.906     0.4\n",
      "1468.1139545440674\n",
      "     49     1  3.9e-04   9.6e+01      98.1    9.4e+01      98.2   0.956   0.904     0.4\n",
      "1495.082905292511\n",
      "     50     1  3.9e-04   9.5e+01      98.2    9.2e+01      98.2   0.958   0.909     0.4\n",
      "1521.3763732910156\n",
      "     51     1  3.5e-04   9.3e+01      98.3    9.1e+01      98.3   0.960   0.915     0.5\n",
      "1548.517688035965\n",
      "     52     1  3.5e-04   9.4e+01      98.1    9.1e+01      98.2   0.960   0.915     0.5\n",
      "1576.3368790149689\n",
      "     53     1  3.5e-04   9.3e+01      98.1    9.2e+01      98.2   0.960   0.911     0.5\n",
      "1605.345195055008\n",
      "     54     1  3.5e-04   9.2e+01      98.2    9.0e+01      98.3   0.959   0.915     0.5\n",
      "1634.5660095214844\n",
      "     55     1  3.5e-04   9.1e+01      98.2    9.0e+01      98.2   0.961   0.915     0.5\n",
      "1663.717318058014\n",
      "     56     1  3.1e-04   9.0e+01      98.2    9.0e+01      98.2   0.960   0.918     0.5\n",
      "1691.7091360092163\n",
      "     57     1  3.1e-04   9.0e+01      98.2    8.8e+01      98.3   0.961   0.921     0.5\n",
      "1720.710578918457\n",
      "     58     1  3.1e-04   8.9e+01      98.2    8.9e+01      98.3   0.963   0.920     0.5\n",
      "1754.1801517009735\n",
      "     59     1  3.1e-04   9.0e+01      98.1    8.8e+01      98.2   0.961   0.918     0.5\n",
      "1787.4271302223206\n",
      "     60     1  3.1e-04   8.8e+01      98.3    8.8e+01      98.2   0.962   0.918     0.5\n",
      "1822.372495174408\n",
      "     61     1  2.8e-04   8.8e+01      98.3    8.8e+01      98.2   0.964   0.926     0.5\n",
      "1855.4991371631622\n",
      "     62     1  2.8e-04   8.7e+01      98.3    8.7e+01      98.3   0.964   0.923     0.5\n",
      "1883.6346509456635\n",
      "     63     1  2.8e-04   8.8e+01      98.2    8.7e+01      98.2   0.964   0.926     0.5\n",
      "1915.8106718063354\n",
      "     64     1  2.8e-04   8.7e+01      98.3    8.7e+01      98.2   0.963   0.925     0.5\n",
      "1948.9685637950897\n",
      "     65     1  2.8e-04   8.7e+01      98.3    8.6e+01      98.2   0.964   0.923     0.5\n",
      "1982.474709033966\n",
      "     66     1  2.5e-04   8.6e+01      98.3    8.5e+01      98.3   0.966   0.931     0.5\n",
      "2012.6994059085846\n",
      "     67     1  2.5e-04   8.4e+01      98.4    8.6e+01      98.3   0.965   0.930     0.5\n",
      "2041.7892570495605\n",
      "     68     1  2.5e-04   8.6e+01      98.2    8.6e+01      98.2   0.965   0.925     0.5\n",
      "2070.756922483444\n",
      "     69     1  2.5e-04   8.6e+01      98.3    8.5e+01      98.3   0.966   0.929     0.5\n",
      "2099.5374705791473\n",
      "     70     1  2.5e-04   8.5e+01      98.3    8.5e+01      98.3   0.965   0.929     0.6\n",
      "2130.2095499038696\n",
      "     71     1  2.3e-04   8.4e+01      98.3    8.4e+01      98.2   0.967   0.932     0.5\n",
      "2158.6610808372498\n",
      "     72     1  2.3e-04   8.4e+01      98.3    8.4e+01      98.3   0.966   0.930     0.5\n",
      "2189.211271762848\n",
      "     73     1  2.3e-04   8.4e+01      98.3    8.4e+01      98.2   0.966   0.933     0.5\n",
      "2217.860680580139\n",
      "     74     1  2.3e-04   8.3e+01      98.3    8.4e+01      98.3   0.967   0.931     0.5\n",
      "2247.3105783462524\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-446dac3a06d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msgvlb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "# Для ускорения обучения используйте отжиг kl \n",
    "# Отжиг кл помогает избежать \"плохих локальных оптимумов\" и устроен так \n",
    "# kl_weight стартует с маленького значения и увеличивается до 1 с каким-то шагом\n",
    "# а потом остается 1 до конца обучения \n",
    "\n",
    "train_loader, test_loader = get_mnist(batch_size)\n",
    "sgvlb = SGVLB(model, len(train_loader.dataset))\n",
    "step = 0.02 # шаг для увеличения kl_weight\n",
    "\n",
    "# ===============\n",
    "kl_weight = step # Начальное значение веса KL\n",
    "# ===============\n",
    "start = time()\n",
    "print(time() - start)\n",
    "for epoch in range(1, epochs + 1):\n",
    "    scheduler.step()\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0 \n",
    "    kl_weight = min(kl_weight+step, 1) \n",
    "    logger.add_scalar(epoch, 'kl', kl_weight)\n",
    "    logger.add_scalar(epoch, 'lr', scheduler.get_lr()[0])\n",
    "    for batch_idx, (data, target) in (enumerate(train_loader)):\n",
    "        data, target = Variable(data).view(-1, 28*28), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "        pred = output.data.max(1)[1] \n",
    "        loss = sgvlb(output, target, kl_weight)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.data.numpy()\n",
    "        train_acc += np.sum(pred.numpy() == target.data.numpy())\n",
    "\n",
    "    logger.add_scalar(epoch, 'tr_los', train_loss / len(train_loader.dataset))\n",
    "    logger.add_scalar(epoch, 'tr_acc', train_acc / len(train_loader.dataset) * 100)\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    test_loss, test_acc = 0, 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        data, target = Variable(data, volatile=True).view(-1, 28*28), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += sgvlb(output, target, kl_weight).data[0]\n",
    "        pred = output.data.max(1)[1] \n",
    "        test_acc += np.sum(pred.numpy() == target.data.numpy())\n",
    "        \n",
    "    logger.add_scalar(epoch, 'te_loss', test_loss / len(test_loader.dataset))\n",
    "    logger.add_scalar(epoch, 'te_acc', test_acc / len(test_loader.dataset) * 100)\n",
    "    \n",
    "    for i, c in enumerate(model.children()):\n",
    "        if hasattr(c, 'kl_reg'):\n",
    "            logger.add_scalar(epoch, 'sp_%s' % i, (c.log_alpha.data.numpy() > threshold).mean())\n",
    "    \n",
    "    logger.iter_info()\n",
    "    print(time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keept weight ratio = 47.30762395592678\n"
     ]
    }
   ],
   "source": [
    "# Посмотрим во сколько раз у нас уменьшилось количество весов в первом слое \n",
    "# Тут хотим получить в 30+ раз меньше весов без падения качества на тесте\n",
    "\n",
    "all_w, kep_w = 0, 0\n",
    "\n",
    "for c in model.children():\n",
    "    kep_w += (c.log_alpha.data.numpy() < 1.5).sum()\n",
    "    all_w += c.log_alpha.data.numpy().size\n",
    "\n",
    "print('keept weight ratio =', all_w/kep_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 876349.1577148438 acc: 9809\n"
     ]
    }
   ],
   "source": [
    "for c in model.children():\n",
    "    c.threshold = 1.5\n",
    "test_loss, test_acc = 0, 0\n",
    "model.eval()\n",
    "for batch_idx, (data, target) in enumerate(test_loader):\n",
    "    data, target = Variable(data, volatile=True).view(-1, 28*28), Variable(target)\n",
    "    output = model(data)\n",
    "    test_loss += sgvlb(output, target, kl_weight).data[0]\n",
    "    pred = output.data.max(1)[1] \n",
    "    test_acc += np.sum(pred.numpy() == target.data.numpy())\n",
    "print('loss:', test_loss, 'acc:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим какая компрессия получилась на диске\n",
    "import scipy\n",
    "import numpy as np\n",
    "from scipy.sparse import csc_matrix, csc_matrix, coo_matrix, dok_matrix\n",
    "\n",
    "row, col, data = [], [], []\n",
    "M = list(model.children())[0].W.data.numpy()\n",
    "LA = list(model.children())[0].log_alpha.data.numpy()\n",
    "\n",
    "for i in range(300):\n",
    "    for j in range(28*28):\n",
    "        if LA[i, j] < 1.5:\n",
    "            row += [i]\n",
    "            col += [j]\n",
    "            data += [M[i, j]]\n",
    "\n",
    "Mcsr = csc_matrix((data, (row, col)), shape=(300, 28*28))\n",
    "Mcsc = csc_matrix((data, (row, col)), shape=(300, 28*28))\n",
    "Mcoo = coo_matrix((data, (row, col)), shape=(300, 28*28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('M_w', M)\n",
    "scipy.sparse.save_npz('Mcsr_w', Mcsr)\n",
    "scipy.sparse.save_npz('Mcsc_w', Mcsc)\n",
    "scipy.sparse.save_npz('Mcoo_w', Mcoo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 dvschelchkov dvschelchkov  22K Mar 28 23:57 Mcoo_w.npz\r\n",
      "-rw-rw-r-- 1 dvschelchkov dvschelchkov  21K Mar 28 23:57 Mcsc_w.npz\r\n",
      "-rw-rw-r-- 1 dvschelchkov dvschelchkov  21K Mar 28 23:57 Mcsr_w.npz\r\n",
      "-rw-rw-r-- 1 dvschelchkov dvschelchkov 840K Mar 28 23:57 M_w.npz\r\n"
     ]
    }
   ],
   "source": [
    "ls -lah | grep _w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бонусная часть баллов 25%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Сжать этим методом сверточную сеть LeNet-5-Caffe в 100 + раз http://caffe.berkeleyvision.org/gathered/examples/mnist.html\n",
    "- Поэкспериментировать с разной битностью весов -- при какой битности качество начинает падать\n",
    "- Помогают ли веса меньшей битности сэкономить место на диске?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, groups=1, bias=True):\n",
    "        kernel_size = _pair(kernel_size)\n",
    "        stride = _pair(stride)\n",
    "        padding = _pair(padding)\n",
    "        dilation = _pair(dilation)\n",
    "        self.log_sigma = Parameter(torch.Tensor(\n",
    "                out_channels, in_channels // groups, *kernel_size))\n",
    "        super(Conv2d, self).__init__(\n",
    "            in_channels, out_channels, kernel_size, stride, padding, dilation,\n",
    "            False, _pair(0), groups, bias)\n",
    "\n",
    "    @property\n",
    "    def log_alpha(self):\n",
    "        return 2 * self.log_sigma - (self.W ** 2 + 1e-8).log()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        n = self.in_channels\n",
    "        for k in self.kernel_size:\n",
    "            n *= k\n",
    "        stdv = 1. / math.sqrt(n)\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "        self.log_sigma.data.uniform_(-5, -5)\n",
    "\n",
    "    def forward(self, input):\n",
    "        eps = Variable(torch.randn((n, self.out_features, ))) \n",
    "        result = F.conv2d(input, self.weight, self.bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "        eps = Variable(torch.randn(result.shape)) \n",
    "        result += F.conv2d(input ** 2, self.log_sigma.clamp(-10, 10).exp() ** 2, self.stride,\n",
    "                        self.padding, self.dilation, self.groups) ** 0.5\n",
    "        return result\n",
    "        \n",
    "    def kl_reg(self):\n",
    "        # =======================================\n",
    "        # Вернуть суммарную KL дивергенцию для всех параметров слоя \n",
    "        # Return: Variable containing: [torch.FloatTensor of size 1]\n",
    "        # =======================================\n",
    "        k1 = 0.63576\n",
    "        k2 = 1.87320\n",
    "        k3 = 1.48695\n",
    "        C = -k1\n",
    "        kl = -(k1 * torch.sigmoid(k2 + k3 * self.log_alpha) - 0.5 * (1 + (-self.log_alpha).exp()).log() + C)\n",
    "        return kl.sum(dim=0).sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, threshold):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = LinearSVDO(28*28, 300, threshold)\n",
    "        self.fc2 = LinearSVDO(300,  100, threshold)\n",
    "        self.fc3 = LinearSVDO(100,  10, threshold)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.log_softmax(self.fc3(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs, batch_size, threshold = 100, 100, 3\n",
    "model = Net(threshold)\n",
    "optimizer = torch.optim.Adam(model.parameters())# Тут ваш любимый оптимизатор, адам -- хороший выбор\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1000000) # Тут расписание шага обучения torch.optim.lr_scheduler\n",
    "fmt = {'tr_los': '3.1e', 'te_loss': '3.1e', 'sp_0': '.3f', 'sp_1': '.3f', 'lr': '3.1e', 'kl': '.2f'}\n",
    "logger = Logger('sparse_vd', fmt=fmt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
