{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Практичиское задание 2: Sparse Variational Dropout</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит реализовать Sparse VD -- метод для разреживания нейронных сетей https://arxiv.org/abs/1701.05369  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from logger import Logger\n",
    "from torch.nn import Parameter\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализуем полносвязный Sparse VD слой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LinearSVDO(nn.Module):\n",
    "    def __init__(self, in_features, out_features, threshold, bias=True):\n",
    "        super(LinearSVDO, self).__init__()\n",
    "        # in_features int\n",
    "        # out_features int \n",
    "        # threshold float\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.threshold = threshold\n",
    "\n",
    "        # =======================================\n",
    "        # Создайте параметры модели -- объекты класса Parameter\n",
    "        # W размера (out_features x in_features)\n",
    "        # log_sigma размера (out_features x in_features)\n",
    "        # bias размера (1, out_features)\n",
    "        # =======================================\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # =======================================\n",
    "        # Инициализируйте параметры модели\n",
    "        # W -- нормальный случайный шум с центром в 0 и маленькой дисперсией\n",
    "        # log_sigma -- маленьким значением ~ -5 \n",
    "        # bias -- можно 0\n",
    "        # =======================================   \n",
    "        self.W = Parameter(torch.randn((self.out_features, self.in_features)))\n",
    "        self.log_sigma = Parameter(torch.zeros((self.out_features, self.in_features)) - 5)\n",
    "        self.bias = Parameter(torch.zeros((1, self.out_features)))\n",
    "    \n",
    "    @property\n",
    "    def log_alpha(self):\n",
    "        return self.log_sigma - (self.W ** 2 + 1e-8).log()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # =======================================\n",
    "        # x: Variable containing: [torch.FloatTensor of size batch_size x in_features]\n",
    "        # Return: type: Variable containing [torch.FloatTensor of size batch_size x out_features]\n",
    "        # ----------------------------------------\n",
    "        # Тут нужно написать forward шаг для Sparse VD слоя для минибатча объектов x\n",
    "        # На этапе обучения: Вернуть семпл активаций с помощью Local Reparametrization Trick \n",
    "        # На этапе тестирования: Вернуть средние активации, посчитанные с обрезанными весами \n",
    "        # Правило обрезания весов: alpha_ij < self.threshold ====> w_ij = 0\n",
    "        # ----------------------------------------\n",
    "        # Клипинг alpha_ij, например torch.clamp(self.log_alpha, -10, 10) может улучшить стабильность \n",
    "        # Чтобы не встретить nan-ы используйте трюк log(a) = log(a + 1e-8)\n",
    "        # ======================================= \n",
    "        \n",
    "        n, _ = x.shape\n",
    "        if self.training: # training\n",
    "            eps = Variable(torch.randn((self.out_features, self.in_features))) \n",
    "            w = self.W.view(self.out_features, self.in_features) +\\\n",
    "                eps * self.log_sigma.clamp(-10, 10).exp()\n",
    "        else:\n",
    "            w = self.W * (self.log_alpha.exp() < (self.threshold + 1e-8)).float()\n",
    "        output = F.linear(x, w, self.bias)\n",
    "        return output\n",
    "        \n",
    "    def kl_reg(self):\n",
    "        # =======================================\n",
    "        # Вернуть суммарную KL дивергенцию для всех параметров слоя \n",
    "        # Return: Variable containing: [torch.FloatTensor of size 1]\n",
    "        # =======================================\n",
    "        k1 = 0.63576\n",
    "        k2 = 1.87320\n",
    "        k3 = 1.48695\n",
    "        C = -k1\n",
    "        kl = k1 * torch.sigmoid(k2 + k3 * self.log_alpha) - 0.5 * (1 + (-self.log_alpha).exp()).log() + C\n",
    "        return kl.sum(dim=0).sum(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создадим простую архитектуру LeNet-300-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, threshold):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = LinearSVDO(28*28, 300, threshold)\n",
    "        self.fc2 = LinearSVDO(300,  100, threshold)\n",
    "        self.fc3 = LinearSVDO(100,  10, threshold)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.log_softmax(self.fc3(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузим MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mnist(batch_size):\n",
    "    trsnform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "        transform=trsnform), batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, download=True,\n",
    "        transform=trsnform), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Определим новую функцию потерь SGVLB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SGVLB(nn.Module):\n",
    "    def __init__(self, net, train_size):\n",
    "        super(SGVLB, self).__init__()\n",
    "        self.train_size = train_size\n",
    "        self.net = net\n",
    "\n",
    "    def forward(self, output, target, kl_weight=1.0):\n",
    "        # =======================================\n",
    "        # output -- ответы модели для минибатча [torch.FloatTensor of size batch_size x 10]\n",
    "        # target -- настоящие ответы  [torch.LongTensor of size batch_size]\n",
    "        # kl_weight -- коэффициент на который нужно умножить kl дивергенцию, нужен для отжига (читай ниже)\n",
    "        # Вернуть Variable с посчитанной SGVLB функцией потерь \n",
    "        # Используйте self.net.children() для обхода всех слоев модели\n",
    "        # !!! Проверьте что множитель перед data term правильный !!!\n",
    "        # Return: Variable containing: [torch.FloatTensor of size 1]\n",
    "        # =======================================\n",
    "        loss = Variable(torch.FloatTensor([0]))\n",
    "        for layer in self.net.children():\n",
    "            loss -= kl_weight * layer.kl_reg()\n",
    "        loss += torch.nn.CrossEntropyLoss()(output, target) * self.train_size / target.shape[0]\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs, batch_size, threshold = 100, 100, 3\n",
    "model = Net(threshold)\n",
    "optimizer = torch.optim.Adam(model.parameters())# Тут ваш любимый оптимизатор, адам -- хороший выбор\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1000000) # Тут расписание шага обучения torch.optim.lr_scheduler\n",
    "fmt = {'tr_los': '3.1e', 'te_loss': '3.1e', 'sp_0': '.3f', 'sp_1': '.3f', 'lr': '3.1e', 'kl': '.2f'}\n",
    "logger = Logger('sparse_vd', fmt=fmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    kl       lr    tr_los    tr_acc    te_loss    te_acc    sp_0    sp_1    sp_2\n",
      "-------  ----  -------  --------  --------  ---------  --------  ------  ------  ------\n",
      "      1  0.02  1.0e-03   2.3e+03      70.8    7.4e+02      85.6   0.015   0.015     0.0\n",
      "      2  0.03  1.0e-03   6.6e+02      87.1    5.7e+02      88.8   0.015   0.014     0.0\n",
      "      3  0.04  1.0e-03   5.5e+02      90.0    5.5e+02      90.3   0.016   0.015     0.0\n",
      "      4  0.05  1.0e-03   5.3e+02      91.9    5.6e+02      91.3   0.017   0.016     0.0\n",
      "      5  0.06  1.0e-03   5.4e+02      93.1    5.9e+02      92.0   0.020   0.018     0.0\n",
      "      6  0.07  1.0e-03   5.6e+02      94.0    6.2e+02      92.6   0.023   0.020     0.0\n",
      "      7  0.08  1.0e-03   5.8e+02      94.9    6.5e+02      93.0   0.029   0.022     0.0\n",
      "      8  0.09  1.0e-03   6.1e+02      95.6    6.8e+02      93.1   0.036   0.027     0.0\n",
      "      9  0.10  1.0e-03   6.4e+02      96.3    7.1e+02      93.6   0.045   0.031     0.0\n",
      "     10  0.11  1.0e-03   6.6e+02      96.7    7.4e+02      93.5   0.057   0.036     0.0\n",
      "     11  0.12  1.0e-03   6.8e+02      97.2    7.6e+02      93.6   0.071   0.046     0.0\n",
      "     12  0.13  1.0e-03   7.0e+02      97.5    7.8e+02      94.0   0.086   0.053     0.0\n",
      "     13  0.14  1.0e-03   7.1e+02      97.8    7.9e+02      94.2   0.102   0.063     0.0\n",
      "     14  0.15  1.0e-03   7.2e+02      98.0    8.0e+02      94.2   0.118   0.072     0.0\n",
      "     15  0.16  1.0e-03   7.3e+02      98.3    8.1e+02      94.5   0.135   0.083     0.0\n",
      "     16  0.17  1.0e-03   7.3e+02      98.5    8.1e+02      94.4   0.153   0.096     0.0\n",
      "     17  0.18  1.0e-03   7.3e+02      98.4    8.1e+02      94.5   0.170   0.107     0.0\n",
      "     21  0.22  1.0e-03   7.1e+02      98.8    7.8e+02      95.0   0.244   0.161     0.0\n",
      "     22  0.23  1.0e-03   7.1e+02      98.8    7.8e+02      95.1   0.262   0.172     0.1\n",
      "     23  0.24  1.0e-03   7.0e+02      98.8    7.6e+02      95.3   0.280   0.188     0.1\n",
      "     24  0.25  1.0e-03   6.9e+02      98.9    7.6e+02      95.3   0.296   0.197     0.1\n",
      "     25  0.26  1.0e-03   6.8e+02      99.0    7.5e+02      95.2   0.312   0.209     0.1\n",
      "     26  0.27  1.0e-03   6.7e+02      99.0    7.4e+02      95.5   0.329   0.222     0.1\n",
      "     27  0.28  1.0e-03   6.6e+02      98.9    7.2e+02      95.5   0.346   0.237     0.1\n",
      "     28  0.29  1.0e-03   6.5e+02      98.9    7.1e+02      95.7   0.361   0.247     0.1\n",
      "     29  0.30  1.0e-03   6.4e+02      99.0    7.0e+02      95.7   0.377   0.261     0.1\n",
      "     30  0.31  1.0e-03   6.3e+02      98.9    6.9e+02      95.6   0.392   0.270     0.1\n",
      "     31  0.32  1.0e-03   6.2e+02      98.9    6.7e+02      95.8   0.407   0.279     0.1\n",
      "     32  0.33  1.0e-03   6.1e+02      99.0    6.6e+02      95.7   0.424   0.295     0.1\n",
      "     33  0.34  1.0e-03   5.9e+02      98.9    6.5e+02      95.8   0.435   0.299     0.1\n",
      "     34  0.35  1.0e-03   5.8e+02      98.9    6.4e+02      95.8   0.449   0.312     0.1\n",
      "     35  0.36  1.0e-03   5.7e+02      98.9    6.2e+02      96.0   0.465   0.323     0.1\n",
      "     36  0.37  1.0e-03   5.6e+02      98.9    6.1e+02      95.9   0.479   0.335     0.1\n",
      "     37  0.38  1.0e-03   5.5e+02      98.8    6.0e+02      96.0   0.492   0.343     0.1\n",
      "     38  0.39  1.0e-03   5.4e+02      98.7    5.8e+02      96.0   0.505   0.355     0.1\n",
      "     39  0.40  1.0e-03   5.3e+02      98.7    5.7e+02      96.0   0.517   0.364     0.1\n",
      "     40  0.41  1.0e-03   5.2e+02      98.6    5.6e+02      96.1   0.528   0.371     0.1\n",
      "     41  0.42  1.0e-03   5.1e+02      98.7    5.5e+02      96.2   0.539   0.381     0.1\n",
      "     42  0.43  1.0e-03   5.0e+02      98.6    5.4e+02      96.3   0.551   0.392     0.1\n",
      "     43  0.44  1.0e-03   4.9e+02      98.6    5.3e+02      96.0   0.562   0.401     0.1\n",
      "     44  0.45  1.0e-03   4.8e+02      98.5    5.2e+02      96.4   0.573   0.409     0.1\n",
      "     45  0.46  1.0e-03   4.7e+02      98.5    5.1e+02      96.3   0.585   0.418     0.1\n",
      "     46  0.47  1.0e-03   4.6e+02      98.4    5.0e+02      96.4   0.594   0.423     0.2\n",
      "     47  0.48  1.0e-03   4.5e+02      98.4    4.8e+02      96.4   0.604   0.433     0.2\n",
      "     48  0.49  1.0e-03   4.4e+02      98.4    4.7e+02      96.4   0.615   0.443     0.2\n",
      "     49  0.50  1.0e-03   4.3e+02      98.3    4.6e+02      96.4   0.625   0.452     0.2\n",
      "     50  0.51  1.0e-03   4.2e+02      98.2    4.5e+02      96.5   0.633   0.454     0.2\n",
      "     51  0.52  1.0e-03   4.2e+02      98.2    4.4e+02      96.6   0.644   0.464     0.2\n",
      "     52  0.53  1.0e-03   4.1e+02      98.1    4.3e+02      96.5   0.653   0.471     0.2\n",
      "     53  0.54  1.0e-03   4.0e+02      98.0    4.2e+02      96.6   0.660   0.477     0.2\n",
      "     54  0.55  1.0e-03   3.9e+02      98.0    4.1e+02      96.5   0.670   0.485     0.2\n",
      "     55  0.56  1.0e-03   3.8e+02      97.9    4.0e+02      96.8   0.678   0.490     0.2\n",
      "     56  0.57  1.0e-03   3.7e+02      97.9    3.9e+02      96.7   0.687   0.501     0.2\n",
      "     57  0.58  1.0e-03   3.7e+02      97.8    3.8e+02      96.7   0.695   0.508     0.2\n",
      "     58  0.59  1.0e-03   3.6e+02      97.7    3.7e+02      96.6   0.703   0.518     0.2\n",
      "     59  0.60  1.0e-03   3.5e+02      97.7    3.6e+02      96.5   0.711   0.525     0.2\n",
      "     60  0.61  1.0e-03   3.4e+02      97.6    3.5e+02      96.7   0.719   0.534     0.2\n",
      "     61  0.62  1.0e-03   3.4e+02      97.6    3.5e+02      96.5   0.727   0.539     0.2\n",
      "     62  0.63  1.0e-03   3.3e+02      97.6    3.4e+02      96.7   0.735   0.550     0.2\n",
      "     63  0.64  1.0e-03   3.2e+02      97.5    3.3e+02      96.7   0.741   0.556     0.2\n",
      "     64  0.65  1.0e-03   3.1e+02      97.5    3.2e+02      96.8   0.749   0.567     0.2\n",
      "     65  0.66  1.0e-03   3.0e+02      97.2    3.1e+02      96.8   0.756   0.575     0.2\n",
      "     66  0.67  1.0e-03   3.0e+02      97.2    3.0e+02      96.8   0.763   0.583     0.2\n",
      "     67  0.68  1.0e-03   2.9e+02      97.0    2.9e+02      96.8   0.770   0.590     0.2\n",
      "     68  0.69  1.0e-03   2.8e+02      96.9    2.8e+02      96.7   0.778   0.596     0.2\n",
      "     69  0.70  1.0e-03   2.7e+02      96.7    2.8e+02      96.7   0.784   0.605     0.2\n",
      "     70  0.71  1.0e-03   2.7e+02      96.6    2.7e+02      96.6   0.789   0.610     0.3\n",
      "     71  0.72  1.0e-03   2.6e+02      96.5    2.6e+02      96.7   0.797   0.620     0.2\n",
      "     72  0.73  1.0e-03   2.5e+02      96.5    2.5e+02      96.7   0.803   0.626     0.3\n",
      "     73  0.74  1.0e-03   2.4e+02      96.3    2.4e+02      96.6   0.809   0.633     0.3\n",
      "     74  0.75  1.0e-03   2.4e+02      96.3    2.4e+02      96.6   0.815   0.640     0.3\n",
      "     75  0.76  1.0e-03   2.3e+02      96.1    2.3e+02      96.6   0.822   0.647     0.3\n",
      "     76  0.77  1.0e-03   2.2e+02      96.1    2.2e+02      96.5   0.827   0.653     0.3\n",
      "     77  0.78  1.0e-03   2.1e+02      95.8    2.1e+02      96.5   0.834   0.664     0.3\n",
      "     78  0.79  1.0e-03   2.1e+02      95.6    2.0e+02      96.3   0.840   0.669     0.3\n",
      "     79  0.80  1.0e-03   2.0e+02      95.5    1.9e+02      96.6   0.847   0.681     0.3\n",
      "     80  0.81  1.0e-03   1.9e+02      95.2    1.9e+02      96.5   0.853   0.687     0.3\n",
      "     81  0.82  1.0e-03   1.9e+02      95.1    1.8e+02      96.4   0.859   0.698     0.3\n",
      "     82  0.83  1.0e-03   1.8e+02      94.8    1.7e+02      96.5   0.865   0.707     0.3\n",
      "     83  0.84  1.0e-03   1.7e+02      94.5    1.6e+02      96.1   0.871   0.713     0.3\n",
      "     84  0.85  1.0e-03   1.6e+02      94.3    1.6e+02      96.0   0.877   0.722     0.3\n",
      "     85  0.86  1.0e-03   1.6e+02      93.9    1.5e+02      95.6   0.882   0.730     0.3\n",
      "     86  0.87  1.0e-03   1.5e+02      93.8    1.4e+02      95.8   0.888   0.739     0.3\n",
      "     87  0.88  1.0e-03   1.4e+02      93.3    1.3e+02      95.6   0.893   0.748     0.4\n",
      "     88  0.89  1.0e-03   1.3e+02      93.0    1.3e+02      95.4   0.899   0.758     0.4\n",
      "     89  0.90  1.0e-03   1.3e+02      92.4    1.2e+02      95.2   0.904   0.767     0.4\n",
      "     90  0.91  1.0e-03   1.2e+02      91.7    1.1e+02      95.1   0.910   0.777     0.4\n",
      "     91  0.92  1.0e-03   1.1e+02      91.2    1.0e+02      94.9   0.915   0.785     0.4\n",
      "     92  0.93  1.0e-03   1.1e+02      90.7    9.8e+01      93.9   0.921   0.794     0.4\n",
      "     93  0.94  1.0e-03   9.9e+01      89.8    9.1e+01      94.0   0.926   0.803     0.4\n",
      "     94  0.95  1.0e-03   9.2e+01      88.9    8.4e+01      93.3   0.931   0.813     0.4\n",
      "     95  0.96  1.0e-03   8.5e+01      87.6    7.7e+01      92.2   0.936   0.823     0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     96  0.97  1.0e-03   7.8e+01      86.1    7.0e+01      91.6   0.941   0.833     0.5\n",
      "     97  0.98  1.0e-03   7.0e+01      84.2    6.3e+01      90.5   0.946   0.841     0.5\n",
      "     98  0.99  1.0e-03   6.3e+01      82.0    5.6e+01      89.0   0.951   0.851     0.5\n",
      "     99     1  1.0e-03   5.7e+01      79.9    5.1e+01      86.3   0.956   0.862     0.5\n",
      "    100     1  1.0e-03   5.0e+01      78.2    4.5e+01      83.8   0.962   0.874     0.6\n"
     ]
    }
   ],
   "source": [
    "# Для ускорения обучения используйте отжиг kl \n",
    "# Отжиг кл помогает избежать \"плохих локальных оптимумов\" и устроен так \n",
    "# kl_weight стартует с маленького значения и увеличивается до 1 с каким-то шагом\n",
    "# а потом остается 1 до конца обучения \n",
    "\n",
    "train_loader, test_loader = get_mnist(batch_size)\n",
    "sgvlb = SGVLB(model, len(train_loader.dataset))\n",
    "step = 0.01 # шаг для увеличения kl_weight\n",
    "\n",
    "# ===============\n",
    "kl_weight = step # Начальное значение веса KL\n",
    "# ===============\n",
    "for epoch in range(1, epochs + 1):\n",
    "    scheduler.step()\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0 \n",
    "    kl_weight = min(kl_weight+step, 1) \n",
    "    logger.add_scalar(epoch, 'kl', kl_weight)\n",
    "    logger.add_scalar(epoch, 'lr', scheduler.get_lr()[0])\n",
    "    for batch_idx, (data, target) in (enumerate(train_loader)):\n",
    "        data, target = Variable(data).view(-1, 28*28), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "        pred = output.data.max(1)[1] \n",
    "        loss = sgvlb(output, target, kl_weight)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss \n",
    "        train_acc += np.sum(pred.numpy() == target.data.numpy())\n",
    "\n",
    "    logger.add_scalar(epoch, 'tr_los', train_loss / len(train_loader.dataset))\n",
    "    logger.add_scalar(epoch, 'tr_acc', train_acc / len(train_loader.dataset) * 100)\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    test_loss, test_acc = 0, 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        data, target = Variable(data, volatile=True).view(-1, 28*28), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += sgvlb(output, target, kl_weight).data[0] \n",
    "        pred = output.data.max(1)[1] \n",
    "        test_acc += np.sum(pred.numpy() == target.data.numpy())\n",
    "        \n",
    "    logger.add_scalar(epoch, 'te_loss', test_loss / len(test_loader.dataset))\n",
    "    logger.add_scalar(epoch, 'te_acc', test_acc / len(test_loader.dataset) * 100)\n",
    "    \n",
    "    for i, c in enumerate(model.children()):\n",
    "        if hasattr(c, 'kl_reg'):\n",
    "            logger.add_scalar(epoch, 'sp_%s' % i, (c.log_alpha.data.numpy() > threshold).mean())\n",
    "            \n",
    "    logger.iter_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Посмотрим во сколько раз у нас уменьшилось количество весов в первом слое \n",
    "# Тут хотим получить в 30+ раз меньше весов без падения качества на тесте\n",
    "\n",
    "all_w, kep_w = 0, 0\n",
    "\n",
    "for c in model.children():\n",
    "    kep_w += (c.log_alpha.data.numpy() < 3).sum()\n",
    "    all_w += c.log_alpha.data.numpy().size\n",
    "\n",
    "print('keept weight ratio =', all_w/kep_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Посмотрим какая компрессия получилась на диске\n",
    "import scipy\n",
    "import numpy as np\n",
    "from scipy.sparse import csc_matrix, csc_matrix, coo_matrix, dok_matrix\n",
    "\n",
    "row, col, data = [], [], []\n",
    "M = list(model.children())[0].W.data.numpy()\n",
    "LA = list(model.children())[0].log_alpha.data.numpy()\n",
    "\n",
    "for i in range(300):\n",
    "    for j in range(28*28):\n",
    "        if LA[i, j] < 3:\n",
    "            row += [i]\n",
    "            col += [j]\n",
    "            data += [M[i, j]]\n",
    "\n",
    "Mcsr = csc_matrix((data, (row, col)), shape=(300, 28*28))\n",
    "Mcsc = csc_matrix((data, (row, col)), shape=(300, 28*28))\n",
    "Mcoo = coo_matrix((data, (row, col)), shape=(300, 28*28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez_compressed('M_w', M)\n",
    "scipy.sparse.save_npz('Mcsr_w', Mcsr)\n",
    "scipy.sparse.save_npz('Mcsc_w', Mcsc)\n",
    "scipy.sparse.save_npz('Mcoo_w', Mcoo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ls -lah | grep _w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бонусная часть баллов 25%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Сжать этим методом сверточную сеть LeNet-5-Caffe в 100 + раз http://caffe.berkeleyvision.org/gathered/examples/mnist.html\n",
    "- Поэкспериментировать с разной битностью весов -- при какой битности качество начинает падать\n",
    "- Помогают ли веса меньшей битности сэкономить место на диске?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
